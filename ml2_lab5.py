# -*- coding: utf-8 -*-
"""ML2_Lab5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyFRlxy2ysjb2DRvPV0IcRmqYQ1gXCGo

Preprocessing
"""

import numpy as np
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/datasets/amazon_alexa.tsv',sep = '\t')

df.info()

df.isnull().sum()

df.describe()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
df = pd.read_csv('/content/drive/MyDrive/datasets/amazon_alexa.tsv', sep='\t')
text_data = df['verified_reviews']
feedback = df['feedback']
labels = [1 if fb == 1 else 0 for fb in feedback]
X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)

"""Tokenizing"""

max_words = 10000
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)
max_seq_length = 100
X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post', truncating='post')

"""Model building"""

y_train = np.array(y_train, dtype=float)
y_test = np.array(y_test, dtype=float)
embedding_dim = 100
epochs = 10
batch_size = 32
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_length))
model.add(LSTM(units=128))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_padded, y_test))

"""Evaluation"""

y_pred = model.predict(X_test_padded)
y_pred = [1 if prob > 0.5 else 0 for prob in y_pred]

target_names = ['Negative', 'Positive']
print(classification_report(y_test, y_pred, target_names=target_names))

"""Parameter tuning"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

X_train, X_test, y_train, y_test = train_test_split(X_train_padded, y_train, test_size=0.2, random_state=42)

def create_lstm_model(learning_rate=0.001, num_units=128):
    model = Sequential()
    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_seq_length))
    model.add(LSTM(units=num_units))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'num_units': [64, 128]
}

best_accuracy = 0
best_params = None
best_model = None

for learning_rate in param_grid['learning_rate']:
    for num_units in param_grid['num_units']:
        model = create_lstm_model(learning_rate=learning_rate, num_units=num_units)
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
        y_pred = model.predict(X_test)
        y_pred = [1 if prob > 0.5 else 0 for prob in y_pred]
        accuracy = accuracy_score(y_test, y_pred)
        print(f"Learning Rate: {learning_rate}, LSTM Units: {num_units}, Accuracy: {accuracy}")
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_params = {'learning_rate': learning_rate, 'num_units': num_units}
            best_model = model

print(f"Best parameters: {best_params}")
print(f"Best accuracy: {best_accuracy}")

y_pred = best_model.predict(X_test)
y_pred = [1 if prob > 0.5 else 0 for prob in y_pred]
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
print(f"Test accuracy with best model: {accuracy}")
print("Classification Report:\n", classification_rep)

"""TextBlob"""

from textblob import TextBlob
from sklearn.metrics import accuracy_score, classification_report

def textblob_classifier(text):
    analysis = TextBlob(str(text))
    if analysis.sentiment.polarity < 0:
        return 0
    else:
        return 1

textblob_predictions = [textblob_classifier(text) for text in X_test]

textblob_accuracy = accuracy_score(y_test, textblob_predictions)

print("LSTM Model Accuracy:", accuracy)
print("TextBlob Classifier Accuracy:", textblob_accuracy)